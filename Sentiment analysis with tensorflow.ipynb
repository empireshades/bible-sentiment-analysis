{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple proof of concept of how we can apply basic nltk techniques to perform sentiment analysis of The Message (MSG) translation of the bible leveraging tensorflow.\n",
    "\n",
    "High level approach:\n",
    "\n",
    "* Create an array of the most frequntly occuring ([lemmatized](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) words from the training samples\n",
    "* Create feature/label sets for positive and negative sentiment data by counting the number of popular words in each sample, from the array created above\n",
    "* Using the above labelled features as inputs, train a 3 layer feedfoward neural network which will output an array containing probability percentages for True and False\n",
    "* We save the model for later use and later run it on the Bible (MSG) saving the results in a sqlite database\n",
    "\n",
    "Note, the basis for this comes largely from [Rachit Mishra's work](https://becominghuman.ai/deep-learning-using-tensorflow-and-nltk-analyzing-corpuss-sentiments-part-1-bec9d6c1051). I amended with the below:\n",
    "\n",
    "* added a method to create the layers of the neural network\n",
    "* saved the model after training for later re-use\n",
    "* logged training results to be able to view in tensorboard\n",
    "* ran the model on every verse from 3 books of the bible\n",
    "    * Ecclesiastes\n",
    "    * Proverbs\n",
    "    * Psalms\n",
    "* saved the results of the above predictions to a sqlite db\n",
    "\n",
    "Also note that the training data comes from a movie review corpus so the accuracy of results against one of the oldest texts in history are questionable at best, even though I chose to use a modern translation of the bible (MSG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:foo\n",
      "DEBUG:__main__:lexicon contains 423 words\n",
      "DEBUG:__main__:Creating feature set for positive\n",
      "DEBUG:__main__:featureset contains 5331 features\n",
      "DEBUG:__main__:Creating feature set for negative\n",
      "DEBUG:__main__:featureset contains 5331 features\n",
      "DEBUG:__main__:features length is 10662\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# max number of lines we want/need to create the frequency array\n",
    "hm_lines = 10000000\n",
    "\n",
    "# lexicon - find all words in pos and neg data sets\n",
    "def create_lexicon(pos, neg):\n",
    "    lexicon = []\n",
    "    for file in [pos, neg]:\n",
    "        with open(file, 'r') as f:\n",
    "            contents = f.readlines()\n",
    "            for l in contents[:hm_lines]:  # upto however many lines we're gonna read\n",
    "                all_words = word_tokenize(l.lower())  # tokenizing words per line\n",
    "                lexicon += list(all_words)\n",
    "\n",
    "    # lemmatize all these words\n",
    "    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "    # stemming them into legitimate words\n",
    "    # Input Vector is going to be that input vector\n",
    "    # ieally - LEXICON should be shortest possible so that\n",
    "    # we can have a decent sized model in terms of 3 layers\n",
    "    # 1000\n",
    "    w_counts = Counter(lexicon)\n",
    "    # this gives us a dictionary like elements\n",
    "    # w_counts = {'the':52000, 'and',:22323} EXAMPLE\n",
    "    l2 = []\n",
    "    for w in w_counts:\n",
    "        if 1000 > w_counts[w] > 50:\n",
    "            l2.append(w)\n",
    "            # because we dont want super common words like 'the' 'and' 'or' etc. - NOT VALUABLE\n",
    "    logger.debug('lexicon contains {0} words'.format(len(l2)))\n",
    "    return l2\n",
    "    # l2 is the final lexicon\n",
    "\n",
    "\n",
    "def sample_handling(sample, lexicon, classification):\n",
    "    featureset = []  # [1 0] pos sentiment [0 1] negative sentiment\n",
    "    with open(sample, 'r') as f:\n",
    "        # parse in each line\n",
    "        contents = f.readlines()\n",
    "        # loop through each word of each line\n",
    "        for l in contents[:hm_lines]:\n",
    "            current_words = word_tokenize(l.lower())\n",
    "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "            features = np.zeros(len(lexicon))\n",
    "            for word in current_words:\n",
    "                if word.lower() in lexicon:\n",
    "                    index_value = lexicon.index(word.lower())\n",
    "                    # like the example discussed earlier\n",
    "                    features[index_value] += 1\n",
    "            features = list(features)\n",
    "            featureset.append([features, classification])\n",
    "        logger.debug('featureset contains {0} features'.format(len(featureset)))\n",
    "\n",
    "    return featureset\n",
    "\n",
    "\n",
    "def create_feature_sets_and_labels(pos, neg, test_size=0.1):\n",
    "    lexicon = create_lexicon(pos, neg)\n",
    "    global_lexicon = lexicon\n",
    "    features = []\n",
    "    logger.debug('Creating feature set for positive')\n",
    "    features += sample_handling('pos.txt', lexicon, [1, 0])\n",
    "    logger.debug('Creating feature set for negative')\n",
    "    features += sample_handling('neg.txt', lexicon, [0, 1])\n",
    "    random.shuffle(features)\n",
    "    logger.debug('features length is {}'.format(len(features)))\n",
    "    # does tf.agrmax([output]) == tf.argmax)[expectations]) was the final question\n",
    "    # this was the question earlier\n",
    "\n",
    "    # want to shuffle bcause that's how NN Model works - it's going to be shifting the\n",
    "    # weights for RNN model to work\n",
    "    features = np.array(features)\n",
    "    testing_size = int(test_size * len(features))\n",
    "\n",
    "    # x is features, y is labels\n",
    "    train_x = list(features[:, 0][:-testing_size])\n",
    "    train_y = list(features[:, 1][:-testing_size])\n",
    "\n",
    "    test_x = list(features[:, 0][:-testing_size:])\n",
    "    test_y = list(features[:, 1][:-testing_size:])\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, lexicon\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create the train/test groups\n",
    "    train_x, train_y, test_x, test_y, global_lexicon = create_feature_sets_and_labels('pos.txt', 'neg.txt')\n",
    "    '''\n",
    "    with open('sentiment_set.pickle', 'wb') as f:\n",
    "        pickle.dump([train_x, train_y, test_x, test_y], f)\n",
    "    '''\n",
    "\n",
    "# now we want to run this through a deep neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 10 loss: 66.7704710364\n",
      "Epoch 1 completed out of 10 loss: 44.6850358844\n",
      "Epoch 2 completed out of 10 loss: 29.4173834622\n",
      "Epoch 3 completed out of 10 loss: 18.2085752636\n",
      "Epoch 4 completed out of 10 loss: 16.9207520336\n",
      "Epoch 5 completed out of 10 loss: 21.1601841375\n",
      "Epoch 6 completed out of 10 loss: 19.8161383942\n",
      "Epoch 7 completed out of 10 loss: 12.5131532773\n",
      "Epoch 8 completed out of 10 loss: 8.32716379315\n",
      "Epoch 9 completed out of 10 loss: 6.76637831237\n",
      "Accuracy: 0.967903\n",
      "sentiment is [[  9.99436796e-01   5.63225010e-04]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from TF_own_data_model import create_feature_sets_and_labels\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "reset_default_graph()\n",
    "\n",
    "# can load the data from the pickle\n",
    "    # or you could just write it down\n",
    "#train_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')\n",
    "    # 3 hidden layers is probably good enough\n",
    "        # no. of classes = 2 (pos and neg)\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "n_classes = 2\n",
    "\n",
    "batch_size = 100  # can do batches of 100 features at a time\n",
    "x = tf.placeholder('float', [None, len(train_x[0])])  # [None by 423]\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def linear(X, n_input, n_output, activation=None, scope=None):\n",
    "    with tf.variable_scope(scope or \"linear\"):\n",
    "        W = tf.get_variable(\n",
    "            name='W',\n",
    "            shape=[n_input, n_output],\n",
    "            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1))\n",
    "        b = tf.get_variable(\n",
    "            name='b',\n",
    "            shape=[n_output],\n",
    "            initializer=tf.constant_initializer())\n",
    "        h = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            h = activation(h)\n",
    "        return h\n",
    "\n",
    "def neural_network(data):\n",
    "    # layer 1\n",
    "    h1 = linear(data, len(train_x[0]), n_nodes_hl1, tf.nn.relu, scope='layer1')\n",
    "    # layer 2\n",
    "    h2 = linear(h1, n_nodes_hl1, n_nodes_hl2, tf.nn.relu, scope='layer2')\n",
    "    # layer 3\n",
    "    h3 = linear(h2, n_nodes_hl2, n_nodes_hl3, tf.nn.relu, scope='layer3')\n",
    "    # output\n",
    "    output = linear(h3, n_nodes_hl3, n_classes, None, scope='output')\n",
    "    # See the names of any operations in the graph\n",
    "    #print([op.name for op in tf.get_default_graph().get_operations()])\n",
    "    return output\n",
    "\n",
    "    # now all we have to do is explain to TF, what to do with this model\n",
    "    # need to specify how we want to run data through that model\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "\n",
    "    # AdamOptimizer seems to automatically adjust the learning rate as opposed to\n",
    "    # GradientDescent's fixed\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "    # Create a saver object to save our graph\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    n_epochs = 10\n",
    "\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        writer = tf.summary.FileWriter('summary/log', graph=sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())  # initializes our variables. Session has now begun.\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0  # we'll calculate the loss as we go\n",
    "\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                #we want to take batches(chunks); take a slice, then another size)\n",
    "                start = i\n",
    "                end = i+batch_size\n",
    "\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                _, c, summary = sess.run([optimizer, cost, summary_op], feed_dict={x: batch_x, y: batch_y})\n",
    "                \n",
    "                # write log\n",
    "                writer.add_summary(summary, epoch * n_epochs + i)\n",
    "                \n",
    "                # Create a checkpoint in every iteration\n",
    "                #saver.save(sess, 'model/model_iter', global_step=epoch)\n",
    "                \n",
    "                epoch_loss += c\n",
    "                i+=batch_size\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of', n_epochs, 'loss:', epoch_loss)\n",
    "\n",
    "        pred_op = tf.nn.softmax(prediction, name='pred_op')\n",
    "        # Save the final model\n",
    "        saver.save(sess, 'model/model_final')\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        \n",
    "        print('Accuracy:', accuracy.eval({x: test_x, y: test_y}))\n",
    "        \n",
    "        # Run classification against data\n",
    "        feed_dict = {x: np.array([test_x[35]]).astype('float32')}\n",
    "        sentiment = sess.run(tf.nn.softmax(prediction), feed_dict)\n",
    "        print('sentiment is {}'.format(sentiment))\n",
    "\n",
    "train_neural_network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_handling(phrase, lexicon=global_lexicon):\n",
    "    featureset = []  # [1 0] pos sentiment [0 1] negative sentiment\n",
    "    current_words = word_tokenize(phrase.lower())\n",
    "    current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "    features = np.zeros(len(lexicon))\n",
    "    for word in current_words:\n",
    "        if word.lower() in lexicon:\n",
    "            index_value = lexicon.index(word.lower())\n",
    "            # like the example discussed earlier\n",
    "            features[index_value] += 1\n",
    "    features = list(features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (Book, Chapter, Verse, Sentiment (0 for neg, 1 for pos), % pos, % neg),
      "('Psalms', 146, 6, 1, 0.5609954595565796, 0.43900448083877563)\n",
      "('Psalms', 146, 9, 1, 0.7307742238044739, 0.26922574639320374)\n",
      "('Psalms', 146, 8, 1, 0.7182794809341431, 0.28172045946121216)\n",
      "('Psalms', 146, 7, 0, 0.28153279423713684, 0.7184671759605408)\n",
      "('Psalms', 146, 1, 0, 0.4697975516319275, 0.5302023887634277)\n",
      "('Psalms', 146, 3, 1, 0.7310504913330078, 0.2689494788646698)\n",
      "('Psalms', 146, 10, 1, 0.7308939695358276, 0.26910606026649475)\n",
      "('Psalms', 146, 4, 1, 0.7147825360298157, 0.28521740436553955)\n",
      "('Psalms', 146, 2, 0, 0.2690059542655945, 0.7309939861297607)\n",
      "('Psalms', 146, 5, 0, 0.2756005525588989, 0.7243994474411011)\n",
      "('Psalms', 30, 9, 0, 0.2689414322376251, 0.7310585975646973)\n",
      "('Psalms', 30, 11, 0, 0.26894405484199524, 0.7310559153556824)\n",
      "('Psalms', 30, 7, 0, 0.2692694365978241, 0.7307305932044983)\n",
      "('Psalms', 30, 12, 0, 0.2692525386810303, 0.7307474613189697)\n",
      "('Psalms', 30, 2, 0, 0.2850539982318878, 0.714945912361145)\n",
      "('Psalms', 30, 5, 1, 0.7158778309822083, 0.284122109413147)\n",
      "('Psalms', 30, 6, 0, 0.26894447207450867, 0.7310555577278137)\n",
      "('Psalms', 30, 1, 0, 0.26894381642341614, 0.7310562133789062)\n",
      "('Psalms', 30, 3, 0, 0.2689414322376251, 0.7310585975646973)\n",
      "('Psalms', 30, 10, 1, 0.7227758169174194, 0.2772241234779358)\n",
      "('Psalms', 30, 4, 1, 0.7196609377861023, 0.2803390622138977)\n",
      "('Psalms', 30, 8, 0, 0.27032819390296936, 0.729671835899353)\n",
      "('Psalms', 78, 14, 0, 0.34177225828170776, 0.6582277417182922)\n",
      "('Psalms', 78, 25, 0, 0.2801930606365204, 0.7198069095611572)\n",
      "('Psalms', 78, 18, 0, 0.34146103262901306, 0.6585389971733093)\n",
      "('Psalms', 78, 42, 0, 0.26930058002471924, 0.730699360370636)\n",
      "('Psalms', 78, 30, 0, 0.32243457436561584, 0.6775653958320618)\n",
      "('Psalms', 78, 13, 0, 0.32973960041999817, 0.6702604293823242)\n",
      "('Psalms', 78, 50, 1, 0.6740779280662537, 0.32592207193374634)\n",
      "('Psalms', 78, 49, 1, 0.7076331377029419, 0.2923668324947357)\n",
      "('Psalms', 78, 56, 0, 0.2689414322376251, 0.7310585975646973)\n",
      "('Psalms', 78, 68, 0, 0.2761525809764862, 0.7238473892211914)\n",
      "('Psalms', 78, 6, 0, 0.4914984703063965, 0.5085015892982483)\n",
      "('Psalms', 78, 69, 1, 0.7192322611808777, 0.2807677090167999)\n",
      "('Psalms', 78, 40, 1, 0.6194130182266235, 0.3805869519710541)\n",
      "('Psalms', 78, 52, 0, 0.2690562307834625, 0.7309437990188599)\n",
      "('Psalms', 78, 3, 1, 0.7310508489608765, 0.26894915103912354)\n",
      "('Psalms', 78, 45, 0, 0.3921365439891815, 0.6078634262084961)\n",
      "('Psalms', 78, 67, 0, 0.27148571610450745, 0.7285142540931702)\n",
      "('Psalms', 78, 53, 0, 0.45204970240592957, 0.5479503273963928)\n",
      "('Psalms', 78, 70, 0, 0.29315415024757385, 0.7068458795547485)\n",
      "('Psalms', 78, 35, 0, 0.2700681686401367, 0.7299318313598633)\n",
      "('Psalms', 78, 23, 0, 0.4936200976371765, 0.5063799619674683)\n",
      "('Psalms', 78, 16, 1, 0.7285434007644653, 0.27145665884017944)\n",
      "('Psalms', 78, 46, 0, 0.26900923252105713, 0.7309908270835876)\n",
      "('Psalms', 78, 44, 0, 0.3388628661632538, 0.6611371636390686)\n",
      "('Psalms', 78, 72, 0, 0.33287230134010315, 0.6671277284622192)\n",
      "('Psalms', 78, 64, 0, 0.42604246735572815, 0.5739575028419495)\n",
      "('Psalms', 78, 17, 0, 0.2721077799797058, 0.7278921604156494)\n",
      "('Psalms', 78, 28, 0, 0.3405147194862366, 0.6594852805137634)\n",
      "('Psalms', 78, 22, 0, 0.2689439654350281, 0.7310559749603271)\n",
      "('Psalms', 78, 63, 0, 0.47843706607818604, 0.5215628743171692)\n",
      "('Psalms', 78, 55, 0, 0.2691316604614258, 0.7308682799339294)\n",
      "('Psalms', 78, 41, 0, 0.2706635892391205, 0.7293364405632019)\n",
      "('Psalms', 78, 32, 1, 0.6446361541748047, 0.35536378622055054)\n",
      "('Psalms', 78, 19, 0, 0.2704622745513916, 0.7295376658439636)\n",
      "('Psalms', 78, 33, 1, 0.730532705783844, 0.269467294216156)\n",
      "('Psalms', 78, 12, 0, 0.318231463432312, 0.6817685961723328)\n",
      "('Psalms', 78, 71, 0, 0.2692302465438843, 0.7307697534561157)\n",
      "('Psalms', 78, 59, 0, 0.2690349817276001, 0.7309650182723999)\n",
      "('Psalms', 78, 7, 1, 0.6877211332321167, 0.3122788071632385)\n",
      "('Psalms', 78, 62, 0, 0.3216548264026642, 0.6783452033996582)\n",
      "('Psalms', 78, 65, 0, 0.2695521414279938, 0.7304478883743286)\n",
      "('Psalms', 78, 66, 0, 0.2954544425010681, 0.7045456171035767)\n",
      "('Psalms', 78, 21, 0, 0.2914816439151764, 0.708518385887146)\n",
      "('Psalms', 78, 29, 1, 0.5349482297897339, 0.46505171060562134)\n",
      "('Psalms', 78, 1, 0, 0.36894142627716064, 0.6310586333274841)\n",
      "('Psalms', 78, 39, 0, 0.26979249715805054, 0.7302075028419495)\n",
      "('Psalms', 78, 57, 0, 0.2692261040210724, 0.7307738661766052)\n",
      "('Psalms', 78, 48, 1, 0.5544776320457458, 0.44552239775657654)\n",
      "('Psalms', 78, 38, 0, 0.26894229650497437, 0.7310577034950256)\n",
      "('Psalms', 78, 34, 0, 0.26909977197647095, 0.730900228023529)\n",
      "('Psalms', 78, 43, 0, 0.36479997634887695, 0.6351999640464783)\n",
      "('Psalms', 78, 47, 0, 0.3890238106250763, 0.6109762191772461)\n",
      "('Psalms', 78, 26, 0, 0.3581501841545105, 0.6418498158454895)\n",
      "('Psalms', 78, 24, 0, 0.2809038460254669, 0.7190961241722107)\n",
      "('Psalms', 78, 58, 1, 0.7240341901779175, 0.2759658098220825)\n",
      "('Psalms', 78, 31, 0, 0.26949238777160645, 0.7305076122283936)\n",
      "('Psalms', 78, 9, 0, 0.2721253037452698, 0.7278746962547302)\n",
      "('Psalms', 78, 11, 1, 0.6557738184928894, 0.344226211309433)\n",
      "('Psalms', 78, 10, 0, 0.2700894773006439, 0.7299105525016785)\n",
      "('Psalms', 78, 27, 0, 0.2690277695655823, 0.7309722304344177)\n",
      "('Psalms', 78, 37, 0, 0.2695106863975525, 0.7304893732070923)\n",
      "('Psalms', 78, 2, 0, 0.35651081800460815, 0.6434891223907471)\n",
      "('Psalms', 78, 5, 0, 0.30065420269966125, 0.6993457674980164)\n",
      "('Psalms', 78, 51, 0, 0.3756905496120453, 0.6243094205856323)\n",
      "('Psalms', 78, 15, 0, 0.31022557616233826, 0.6897743940353394)\n",
      "('Psalms', 78, 36, 0, 0.26895102858543396, 0.7310490012168884)\n",
      "('Psalms', 78, 54, 0, 0.32144051790237427, 0.6785594820976257)\n",
      "('Psalms', 78, 61, 1, 0.6598232984542847, 0.3401767611503601)\n",
      "('Psalms', 78, 20, 0, 0.27026069164276123, 0.7297393083572388)\n",
      "('Psalms', 78, 4, 0, 0.2705968916416168, 0.7294030785560608)\n",
      "('Psalms', 78, 8, 0, 0.2706241011619568, 0.729375958442688)\n",
      "('Psalms', 78, 60, 0, 0.2690097689628601, 0.7309902310371399)\n",
      "('Psalms', 147, 14, 1, 0.7301867604255676, 0.26981329917907715)\n",
      "('Psalms', 147, 10, 0, 0.2860950231552124, 0.7139049172401428)\n",
      "('Psalms', 147, 18, 1, 0.6944651007652283, 0.30553486943244934)\n",
      "('Psalms', 147, 9, 0, 0.36513078212738037, 0.6348692774772644)\n",
      "('Psalms', 147, 11, 1, 0.5404576063156128, 0.45954233407974243)\n",
      "('Psalms', 147, 7, 1, 0.7282124757766724, 0.27178752422332764)\n",
      "('Psalms', 147, 13, 0, 0.2733529806137085, 0.7266470193862915)\n",
      "('Psalms', 147, 12, 0, 0.4337629973888397, 0.5662369728088379)\n",
      "('Psalms', 147, 16, 0, 0.3027421236038208, 0.6972578763961792)\n",
      "('Psalms', 147, 2, 1, 0.7304688096046448, 0.2695311903953552)\n",
      "('Psalms', 147, 19, 0, 0.30489203333854675, 0.6951079368591309)\n",
      "('Psalms', 147, 5, 1, 0.6561656594276428, 0.3438343107700348)\n",
      "('Psalms', 147, 6, 1, 0.7309380173683167, 0.26906195282936096)\n",
      "('Psalms', 147, 15, 0, 0.2760167717933655, 0.7239832878112793)\n",
      "('Psalms', 147, 17, 0, 0.27911511063575745, 0.7208847999572754)\n",
      "('Psalms', 147, 1, 1, 0.7298208475112915, 0.2701791822910309)\n",
      "('Psalms', 147, 3, 0, 0.30512580275535583, 0.6948741674423218)\n",
      "('Psalms', 147, 20, 0, 0.2704714834690094, 0.729528546333313)\n",
      "('Psalms', 147, 4, 0, 0.3667425215244293, 0.6332574486732483)\n",
      "('Psalms', 147, 8, 1, 0.6222991943359375, 0.3777008652687073)\n",
      "('Psalms', 131, 1, 0, 0.2691582143306732, 0.7308418154716492)\n",
      "('Psalms', 131, 3, 1, 0.7247042655944824, 0.2752957046031952)\n",
      "('Psalms', 131, 2, 1, 0.7289221286773682, 0.27107781171798706)\n",
      "('Psalms', 13, 6, 0, 0.4606076180934906, 0.5393924117088318)\n",
      "('Psalms', 13, 1, 0, 0.26894253492355347, 0.7310574650764465)\n",
      "('Psalms', 13, 3, 1, 0.7308167219161987, 0.2691832482814789)\n",
      "('Psalms', 13, 4, 0, 0.26894691586494446, 0.7310530543327332)\n",
      "('Psalms', 13, 2, 0, 0.2692486047744751, 0.7307513952255249)\n",
      "('Psalms', 13, 5, 1, 0.7303846478462219, 0.2696152925491333)\n",
      "('Psalms', 85, 9, 1, 0.7237957715988159, 0.2762041389942169)\n",
      "('Psalms', 85, 11, 1, 0.6040915846824646, 0.3959084451198578)\n",
      "('Psalms', 85, 7, 1, 0.7310574054718018, 0.26894259452819824)\n",
      "('Psalms', 85, 13, 0, 0.30788400769233704, 0.6921160221099854)\n",
      "('Psalms', 85, 12, 1, 0.6550934910774231, 0.3449064791202545)\n",
      "('Psalms', 85, 2, 1, 0.7310420870780945, 0.26895785331726074)\n",
      "('Psalms', 85, 5, 1, 0.724243700504303, 0.275756299495697)\n",
      "('Psalms', 85, 6, 0, 0.30038130283355713, 0.6996186971664429)\n",
      "('Psalms', 85, 1, 1, 0.7310560345649719, 0.2689439058303833)\n",
      "('Psalms', 85, 3, 0, 0.30932366847991943, 0.6906763315200806)\n",
      "('Psalms', 85, 10, 0, 0.48091036081314087, 0.5190896391868591)\n",
      "('Psalms', 85, 4, 0, 0.42991119623184204, 0.570088803768158)\n",
      "('Psalms', 85, 8, 1, 0.6880764365196228, 0.3119235634803772)\n",
      "('Psalms', 55, 14, 1, 0.6761413216590881, 0.32385873794555664)\n",
      "('Psalms', 55, 18, 0, 0.28381073474884033, 0.7161893248558044)\n",
      "('Psalms', 55, 19, 0, 0.2742129862308502, 0.7257870435714722)\n",
      "('Psalms', 55, 7, 1, 0.7268243432044983, 0.2731756865978241)\n",
      "('Psalms', 55, 13, 1, 0.6416069269180298, 0.35839298367500305)\n",
      "('Psalms', 55, 12, 0, 0.27126264572143555, 0.7287373542785645)\n",
      "('Psalms', 55, 21, 0, 0.26908910274505615, 0.7309108972549438)\n",
      "('Psalms', 55, 6, 0, 0.2717937231063843, 0.728206217288971)\n",
      "('Psalms', 55, 17, 0, 0.3034939765930176, 0.6965059638023376)\n",
      "('Psalms', 55, 3, 0, 0.3194781541824341, 0.6805218458175659)\n",
      "('Psalms', 55, 20, 1, 0.7277992367744446, 0.2722007930278778)\n",
      "('Psalms', 55, 9, 0, 0.26910167932510376, 0.7308983206748962)\n",
      "('Psalms', 55, 23, 1, 0.5200731158256531, 0.47992685437202454)\n",
      "('Psalms', 55, 11, 0, 0.40691691637039185, 0.5930830240249634)\n",
      "('Psalms', 55, 16, 0, 0.27024057507514954, 0.7297594547271729)\n",
      "('Psalms', 60, 9, 0, 0.3002960681915283, 0.6997038722038269)\n",
      "('Psalms', 60, 11, 1, 0.7302204966545105, 0.2697795629501343)\n",
      "('Psalms', 60, 7, 1, 0.6013320088386536, 0.39866796135902405)\n",
      "('Psalms', 60, 12, 1, 0.7264146208763123, 0.27358531951904297)\n",
      "('Psalms', 60, 2, 0, 0.27725234627723694, 0.7227476239204407)\n",
      "('Psalms', 60, 5, 1, 0.7010348439216614, 0.298965185880661)\n",
      "('Psalms', 60, 6, 0, 0.2760186791419983, 0.7239813208580017)\n",
      "('Psalms', 60, 1, 1, 0.6383330821990967, 0.3616669476032257)\n",
      "('Psalms', 60, 3, 1, 0.7309247255325317, 0.26907527446746826)\n",
      "('Psalms', 60, 10, 0, 0.27065911889076233, 0.7293408513069153)\n",
      "('Psalms', 60, 4, 1, 0.6070455312728882, 0.39295443892478943)\n",
      "('Psalms', 60, 8, 1, 0.6678017377853394, 0.33219826221466064)\n",
      "Results truncated .."
     ]
    }
   ],
   "source": [
    "def run_prediction(phrase):\n",
    "    # Parse in MSG bible\n",
    "    with open('MSG.json','r') as foo:\n",
    "        msg = foo.read()\n",
    "        msg = json.loads(msg)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Create a saver object to save our graph\n",
    "        saver = tf.train.import_meta_graph('model/model_final.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('model/'))\n",
    "        #sess.run(tf.global_variables_initializer())  # initializes our variables. Session has now begun.\n",
    "\n",
    "        # Get default graph (supply your custom graph if you have one)\n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        pred_op = graph.get_tensor_by_name(\"pred_op:0\")\n",
    "        \n",
    "        verses = []\n",
    "        \n",
    "        '''\n",
    "        sqlite> .schema bible\n",
    "        CREATE TABLE bible (book text, chapter int, verse int, sentiment int, pos real, neg real);\n",
    "        '''\n",
    "\n",
    "        import sqlite3\n",
    "        conn = sqlite3.connect('bible.db')\n",
    "        c = conn.cursor()\n",
    "        \n",
    "        books = ('Psalms',)\n",
    "        \n",
    "        for book in books:\n",
    "            for chap in msg[book]:\n",
    "                for verse in msg[book][chap].items():\n",
    "                    verse_info = {'chapter': chap,\n",
    "                                  'verse': verse[0],\n",
    "                                  'content': verse[1]}\n",
    "\n",
    "                    verse_info['content'] = np.array([sample_handling(verse_info['content'])]).astype('float32')\n",
    "                    feed_dict = {x: verse_info['content']}\n",
    "                    sentiment = sess.run(tf.nn.softmax(pred_op), feed_dict)\n",
    "                    negpos = {0:'negative', 1:'positive'}\n",
    "                    summary = (book,\n",
    "                               int(verse_info['chapter']),\n",
    "                               int(verse_info['verse']),\n",
    "                               int(sess.run(tf.argmin(sentiment,1))),\n",
    "                               sentiment.tolist()[0][0],\n",
    "                               sentiment.tolist()[0][1]\n",
    "                              )\n",
    "                    print(summary)\n",
    "                    c.execute('INSERT INTO bible VALUES (?,?,?,?,?,?)', summary)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "run_prediction('x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
